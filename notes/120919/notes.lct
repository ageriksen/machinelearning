

finalize topics on errors, how to deal, resampling

1) bootstrap, 2) jack knife, 3) brute force

1) ideal method to asse variance and std. deviation w/no formula
    when less useful? why cross validation the std. method?

project writing tips - email with sample
> like as scientific report


// Errors & resampling methods //
piet heim - to err

don't now prop distribution shape
   

split data - into what? 

discussion boundd by regression -> MSE

label split data k = 1, 2, ..., M -> various ways to proportion out the 2

Bold E w/ double stroke, expectation value

cross-validation evaluates E_rr


E_rr_training

High bias - Low variance / high variance - low bias ???
    o   overfitting


regression jupyter

recommend
get rid of intercept - rescale and normalize data

latter: find and subtract mean value of your data.

preprocessing? 


 /Cross-validation(CV)/

Is there a deeper insight into which k's are good to use?


Leave-One-Out CV? - what is k here? 

Err(CV, K)

for building own code, start simple - use scikitlearn - then build own code

Ridge and Lasso, with hyperparameter lambda
Err(CV, lambda, K)

/////// btrak //////////

//bootstrap//
    - sentral limit theorem? 

recovering correct mean, std devi. etc.
why not much used in ml?

//finding "exact"/"correct" value for std. dev. //

i.i.d. events??? and why bootstrap so good

p.d.f abbreviation? - probability distribution function

    -beggin beyesian analysis

frequentist statisrical approach?

select with replacement?

no split of datam use full data set. Why this relevant to ml?

bootstrap estimate vs. CV. Why is CV better?

Overfitting - even if you have good MSE?

// end of resampling methods //

the lambda thingies ?????????????????????????
////////OLS, Ridge and Lasso/////////7
short recap:

OLS: optimal beta = min beta MSE

Ridge: optimal beta = min beta MSE + lambda||beta||^2_2

Lasso: opt beta = ---------- || --- + lambda||beta||_1

OLS, Ridge -> matrix
Lasso -> not matrix -> scrikit learn (cause gradial descent)


derivative with regards to  beta_j <- function to minimize

tuning lambda -> low bias, low variance

=========
keep in mind: 
    often lacking data -> resampling gets better estimates for fitting MSE

    nias, variance often used for estimation. 
=========

report writing:
TELL A STORY
make your science reproducable
picture worth a thousand words
make an honest critique
